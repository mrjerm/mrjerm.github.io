[
  {
    "objectID": "project_kintoun.html",
    "href": "project_kintoun.html",
    "title": "Project Kintoun: A Regime-Filtered ORB Strategy",
    "section": "",
    "text": "Project Kintoun is an event-driven algorithmic trading strategy designed for the Nasdaq-100 (NQ) futures market.\nWhile standard Opening Range Breakout (ORB) strategies suffer from false breakouts in high-volatility environments, this research attempts to improve risk-adjusted returns and implementing a “Break & Retest” microstructure filter combined with a VWAP trend confirmation.\n\nInstrument: NQ Futures (1-minute timeframe)\nCore Logic: 15-minute ORB with Volume & VWAP confirmation\nData Handling: timezone-aware alignment (US/Eastern) and specific exclusion of degraded data days"
  },
  {
    "objectID": "project_kintoun.html#executive-summary",
    "href": "project_kintoun.html#executive-summary",
    "title": "Project Kintoun: A Regime-Filtered ORB Strategy",
    "section": "",
    "text": "Project Kintoun is an event-driven algorithmic trading strategy designed for the Nasdaq-100 (NQ) futures market.\nWhile standard Opening Range Breakout (ORB) strategies suffer from false breakouts in high-volatility environments, this research attempts to improve risk-adjusted returns and implementing a “Break & Retest” microstructure filter combined with a VWAP trend confirmation.\n\nInstrument: NQ Futures (1-minute timeframe)\nCore Logic: 15-minute ORB with Volume & VWAP confirmation\nData Handling: timezone-aware alignment (US/Eastern) and specific exclusion of degraded data days"
  },
  {
    "objectID": "project_kintoun.html#hypothesis",
    "href": "project_kintoun.html#hypothesis",
    "title": "Project Kintoun: A Regime-Filtered ORB Strategy",
    "section": "1. Hypothesis",
    "text": "1. Hypothesis\nThe first hour of the US Equity session (9:30AM - 10:30AM ET) represent the period of highest price discovery and volatility. A standard ORB strategy bets that a breakout of the initial range will lead to a trend day.\nHowever, blind breakouts are susceptible to false breakouts, where price sweeps an extrema of the opening range, but fails to hold outside.\nMy hypothesis:\n1. Microstructure: Waiting for a retest of the breakout level (e.g. price breaks up → returns to range high → holds) validates support.\n2. Regime: Trades should only be taken if the intraday trend agrees with the Session VWAP slope.\n3. Volume: Breakouts must be accompanied by volume exceeding the 20-period SMA (Simple Moving Average) to confirm aggressive participation."
  },
  {
    "objectID": "project_kintoun.html#data-engineering-challenges",
    "href": "project_kintoun.html#data-engineering-challenges",
    "title": "Project Kintoun: A Regime-Filtered ORB Strategy",
    "section": "2. Data Engineering Challenges",
    "text": "2. Data Engineering Challenges\nOne of the most critical aspects of this backtest was ensuring data integrity. Financial time series data often contains gaps or timezone inconsistencies that can ruin a backtest.\n\nTimezone Alignment\nFutures data often comes in UTC or CST. To accurately capture the 9:30 AM NY Open, I implemented robust timezone handling to account for Daylight Savings Time shifts automatically.\n\n\nVoided Dates\nReal-world data is rarely perfect. I sourced my data from DataBento, and while it was excellent for the most part, I identified specific dates where data feed degradation occured (e.g. gaps in time, outages) and voided those from the simulation."
  },
  {
    "objectID": "project_kintoun.html#mathematical-implementation",
    "href": "project_kintoun.html#mathematical-implementation",
    "title": "Project Kintoun: A Regime-Filtered ORB Strategy",
    "section": "3. Mathematical Implementation",
    "text": "3. Mathematical Implementation\n\nRobust VWAP Calculation\nI calculated the Volume Weighted Average Price (VWAP) using a vectorized approach (NumPy/Pandas) rather than an iterative loop for performance. \\[\n\\text{VWAP} = \\frac{\\sum_jP_j\\cdot{}V_j}{\\sum_jV_j}\n\\] The standard deviation bands (2.0 SD) were calculated to define the “Value Area”\n\n\nEvent Driven Simulation\nWhile vectorization is preferred for indicators, I utilized an Event-Driven Loop for the trade execution logic. This allows for more accurate state management of:\n1. Breakout Status (broke_up / broke_down)\n2. Retest Validation (retest_ok)\n3. Dynamic Position Sizing (Scaling risk based on Equity)"
  },
  {
    "objectID": "project_kintoun.html#simulation-code",
    "href": "project_kintoun.html#simulation-code",
    "title": "Project Kintoun: A Regime-Filtered ORB Strategy",
    "section": "4. Simulation Code",
    "text": "4. Simulation Code\nNote: The full source code handles data loading, indicator calculation, and the backtesting loop. Specific parameter values have been sanitized for public display.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import time, timedelta\nimport pytz\nimport math\n\n# ==========================================\n# 1. CONFIGURATION\n# ==========================================\nCSV_PATH = 'NQ_1m.csv'\n\n# --- DATA SETTINGS ---\nDATA_SOURCE_TYPE = 'FIXED_EST' \n# Void dates (Degraded data)\nVOID_DATES_STR = {\n    '2017-11-13', '2018-10-21', '2019-01-15', '2019-02-22',\n    '2019-03-13', '2019-03-26', '2020-02-27', '2020-02-28',\n    '2020-06-30', '2020-07-01', '2021-12-05', '2022-01-02',\n    '2025-09-17', '2025-09-24', '2025-11-28'\n}\nVOID_DATES = {pd.to_datetime(d).date() for d in VOID_DATES_STR}\n\nSTART_DATE = '2022-06-01'\nEND_DATE = '2025-11-01'\n\n# --- ACCOUNT SETTINGS ---\nINITIAL_CAPITAL = 200_000\nCOMMISSION_PER_CONTRACT = 2.74\nPOINT_VALUE = 20.0 \nRISK_FREE_RATE = 0.04\n\n# --- SIZING SETTINGS ---\n# If True, size scales with equity (200k = 1.0, 220k = 1.1, etc.)\n# increments of 0.1 (approx 1 micro per 20k equity variance)\nENABLE_DYNAMIC_SIZING = True \n\n# If Dynamic is False, this static scale is used\nBASELINE_SCALE_FACTOR = 2\n\n# --- STRATEGY PARAMS ---\nVOL_MULT_LONG = 0\nVOL_MULT_SHORT = 0\nVWAP_REG_LEN = 1\nMAX_TRADES_PER_DAY = 1\nMAX_VWAP_DEV = 1\nENABLE_5M_EXIT = True\nENABLE_NY_VWAP_EXIT = True\n\n# --- TIME SETTINGS ---\nTIME_ORB_START = time(9, 30)\nTIME_ORB_END = time(9, 45)\nTIME_TRADE_START = time(9, 45)\nTIME_TRADE_END = time(14, 0)\nTIME_EOD_EXIT = time(14, 59)\n\nANCHOR_SESSION = time(18, 0)\nANCHOR_NY = time(9, 30)\n\n# ==========================================\n# 2. HELPER FUNCTIONS\n# ==========================================\n\ndef load_and_prep_data(path, date_col='datetime', tz_type='WALL_TIME'):\n    print(f\"Loading {path}...\")\n    try:\n        df = pd.read_csv(path)\n        df.columns = df.columns.str.lower().str.strip()\n        df[date_col] = pd.to_datetime(df[date_col])\n        df.set_index(date_col, inplace=True)\n        df.sort_index(inplace=True)\n        \n        target_tz = pytz.timezone('America/New_York')\n        \n        if tz_type == 'FIXED_EST':\n            df.index = df.index.tz_localize('Etc/GMT+5')\n            df.index = df.index.tz_convert(target_tz)\n        elif tz_type == 'UTC':\n            df.index = df.index.tz_localize('UTC')\n            df.index = df.index.tz_convert(target_tz)\n        elif tz_type == 'WALL_TIME':\n            df.index = df.index.tz_localize(target_tz, ambiguous='infer')\n            \n        return df.dropna()\n    except Exception as e:\n        print(f\"Error loading data: {e}\")\n        return pd.DataFrame()\n\ndef calculate_vwap_robust(df, anchor_time):\n    df = df.copy()\n    minutes = df.index.hour * 60 + df.index.minute\n    anchor_min = anchor_time.hour * 60 + anchor_time.minute\n    \n    day_change = pd.Series(df.index.date).diff().values != pd.Timedelta(0)\n    day_change[0] = True \n    \n    is_after_anchor = minutes &gt;= anchor_min\n    is_after_anchor_prev = np.roll(is_after_anchor, 1)\n    is_after_anchor_prev[0] = False\n    time_cross = (is_after_anchor & ~is_after_anchor_prev)\n    \n    if anchor_time.hour == 9:\n        reset_mask = day_change | time_cross\n    else:\n        reset_mask = time_cross\n        reset_mask[0] = True\n\n    group_id = reset_mask.cumsum()\n    df['hlc3'] = (df['high'] + df['low'] + df['close']) / 3\n    df['pv'] = df['hlc3'] * df['volume']\n    grouper = df.groupby(group_id)\n    cum_vol = grouper['volume'].cumsum()\n    cum_pv = grouper['pv'].cumsum()\n    vwap = cum_pv / cum_vol\n    df['p2v'] = (df['hlc3'] ** 2) * df['volume']\n    cum_p2v = grouper['p2v'].cumsum()\n    variance = (cum_p2v / cum_vol) - (vwap ** 2)\n    stdev = np.sqrt(variance.clip(lower=0))\n    return vwap, stdev\n\ndef calculate_slope(series, window=100):\n    y = series.values\n    x = np.arange(window)\n    sum_x = x.sum()\n    sum_x2 = (x**2).sum()\n    divisor = window * sum_x2 - sum_x**2\n    slopes = np.full(len(y), np.nan)\n    if len(y) &gt; window:\n        strides = (y.strides[0], y.strides[0])\n        shape = (len(y) - window + 1, window)\n        y_strided = np.lib.stride_tricks.as_strided(y, shape=shape, strides=strides)\n        sum_y = y_strided.sum(axis=1)\n        sum_xy = (y_strided * x).sum(axis=1)\n        slopes[window-1:] = (window * sum_xy - sum_x * sum_y) / divisor\n    return pd.Series(slopes, index=series.index)\n\ndef get_base_qty(t):\n    return 1\n\ndef analyze_performance(trades_df, price_df):\n    if trades_df.empty: return\n    \n    min_date = trades_df['exit_ts'].min()\n    max_date = trades_df['exit_ts'].max()\n    price_df_filtered = price_df[(price_df.index &gt;= min_date) & (price_df.index &lt;= max_date)]\n    \n    daily_pnl = trades_df.groupby(trades_df['exit_ts'].dt.date)['pnl'].sum()\n    daily_pnl.index = pd.to_datetime(daily_pnl.index)\n    \n    # Strip timezone for alignment\n    price_df_daily = price_df_filtered['close'].resample('B').last()\n    price_df_daily.index = price_df_daily.index.tz_localize(None).normalize()\n    \n    all_dates = pd.date_range(daily_pnl.index.min(), daily_pnl.index.max(), freq='B')\n    daily_pnl = daily_pnl.reindex(all_dates).fillna(0)\n    price_df_daily = price_df_daily.reindex(all_dates).ffill()\n    \n    benchmark_rets = price_df_daily.pct_change().fillna(0)\n    strat_rets = daily_pnl / INITIAL_CAPITAL\n    aligned_df = pd.DataFrame({'strat': strat_rets, 'bench': benchmark_rets}).dropna()\n    \n    total_trades = len(trades_df)\n    gross_win = trades_df[trades_df['pnl'] &gt; 0]['pnl'].sum()\n    gross_loss = trades_df[trades_df['pnl'] &lt;= 0]['pnl'].sum()\n    profit_factor = abs(gross_win / gross_loss) if gross_loss != 0 else np.inf\n    win_rate = (trades_df['pnl'] &gt; 0).mean()\n    \n    ANN_FACTOR = 252\n    avg_daily_ret = strat_rets.mean()\n    std_daily_ret = strat_rets.std()\n    annualized_ret = avg_daily_ret * ANN_FACTOR\n    \n    sharpe = (avg_daily_ret / std_daily_ret) * np.sqrt(ANN_FACTOR) if std_daily_ret != 0 else 0\n    \n    downside_rets = strat_rets[strat_rets &lt; 0]\n    downside_std = downside_rets.std()\n    sortino = (avg_daily_ret / downside_std) * np.sqrt(ANN_FACTOR) if downside_std != 0 else 0\n    \n    full_equity = daily_pnl.cumsum() + INITIAL_CAPITAL\n    running_max = full_equity.cummax()\n    drawdown_pct = (full_equity - running_max) / running_max\n    max_dd_pct = drawdown_pct.min()\n    max_dd_amt = (full_equity - running_max).min()\n    calmar = annualized_ret / abs(max_dd_pct) if max_dd_pct != 0 else 0\n    \n    if not aligned_df.empty:\n        covariance = aligned_df.cov().iloc[0, 1]\n        bench_var = aligned_df['bench'].var()\n        beta = covariance / bench_var if bench_var != 0 else 0\n        treynor = (annualized_ret - RISK_FREE_RATE) / beta if abs(beta) &gt; 0.0001 else 0\n    else:\n        beta = 0; treynor = 0\n\n    print(\"\\n\" + \"=\"*45)\n    print(f\" KINTOUN PERFORMANCE ({START_DATE} to {END_DATE})\")\n    print(f\" Dynamic Sizing: {ENABLE_DYNAMIC_SIZING}\")\n    print(\"=\"*45)\n    print(f\"Total Trades:      {total_trades}\")\n    print(f\"Win Rate:          {win_rate:.2%}\")\n    print(f\"Profit Factor:     {profit_factor:.2f}\")\n    print(\"-\" * 45)\n    print(f\"Total PnL:         ${trades_df['pnl'].sum():,.2f}\")\n    print(f\"Final Equity:      ${trades_df['equity'].iloc[-1]:,.2f}\")\n    print(f\"Annualized Ret:    {annualized_ret:.2%}\")\n    print(\"-\" * 45)\n    print(f\"Max Drawdown:      {max_dd_pct:.2%} (${max_dd_amt:,.2f})\")\n    print(f\"Sharpe Ratio:      {sharpe:.2f}\")\n    print(f\"Sortino Ratio:     {sortino:.2f}\")\n    print(f\"Calmar Ratio:      {calmar:.2f}\")\n    print(f\"Treynor Ratio:     {treynor:.2f} (Beta: {beta:.2f})\")\n    print(\"=\"*45)\n    \n    print(\"\\n[EXIT REASON DISTRIBUTION]\")\n    print(trades_df['reason'].value_counts())\n    \n    plt.style.use('bmh')\n    fig = plt.figure(figsize=(12, 10))\n    gs = fig.add_gridspec(3, 1, height_ratios=[3, 1, 1])\n    ax1 = fig.add_subplot(gs[0])\n    ax1.plot(full_equity.index, full_equity, label='Strategy Equity', color='#2ecc71')\n    if not aligned_df.empty:\n        bench_equity = (1 + aligned_df['bench']).cumprod() * INITIAL_CAPITAL\n        ax1.plot(bench_equity.index, bench_equity, label='NQ Buy & Hold', color='gray', alpha=0.3, linestyle='--')\n    ax1.set_title(f'Equity Curve')\n    ax1.legend()\n    ax1.set_ylabel('Capital ($)')\n    ax2 = fig.add_subplot(gs[1], sharex=ax1)\n    ax2.fill_between(drawdown_pct.index, drawdown_pct, 0, color='#e74c3c', alpha=0.5)\n    ax2.set_title('Drawdown (%)')\n    ax3 = fig.add_subplot(gs[2], sharex=ax1)\n    ax3.bar(strat_rets.index, strat_rets, color='#3498db', alpha=0.7)\n    ax3.set_title('Daily Returns')\n    plt.tight_layout()\n    plt.show()\n\n# ==========================================\n# 4. MAIN EXECUTION\n# ==========================================\n\ndef run_strategy():\n    df = load_and_prep_data(CSV_PATH, tz_type=DATA_SOURCE_TYPE)\n    if df.empty: return\n\n    print(\"Calculating Indicators...\")\n    df['session_vwap'], _ = calculate_vwap_robust(df, ANCHOR_SESSION)\n    df['ny_vwap'], df['ny_std'] = calculate_vwap_robust(df, ANCHOR_NY)\n    df[['session_vwap','ny_vwap','ny_std']] = df[['session_vwap','ny_vwap','ny_std']].ffill()\n    df['upper'] = df['ny_vwap'] + (MAX_VWAP_DEV * df['ny_std'])\n    df['lower'] = df['ny_vwap'] - (MAX_VWAP_DEV * df['ny_std'])\n    df['slope'] = calculate_slope(df['session_vwap'], window=VWAP_REG_LEN)\n    df['vol_sma'] = df['volume'].rolling(20).mean()\n\n    print(f\"Filtering data from {START_DATE} to {END_DATE}...\")\n    if START_DATE:\n        df = df[df.index &gt;= pd.to_datetime(START_DATE).tz_localize('America/New_York')]\n    if END_DATE:\n        df = df[df.index &lt; pd.to_datetime(END_DATE).tz_localize('America/New_York') + timedelta(days=1)]\n    \n    if df.empty:\n        print(\"Error: No data in date range.\")\n        return\n\n    print(f\"Simulation Range: {df.index[0]} to {df.index[-1]}\")\n    \n    trades = []\n    active = None\n    \n    # Track Equity State\n    current_equity = INITIAL_CAPITAL\n    \n    curr_date = None\n    orb_h, orb_l = np.nan, np.nan\n    broke_up, broke_dn = False, False\n    retest_ok = False\n    break_idx = 0\n    day_trades = 0\n    \n    # Pre-fetch numpy arrays\n    idx_arr = df.index\n    open_arr = df['open'].values\n    high_arr = df['high'].values\n    low_arr = df['low'].values\n    close_arr = df['close'].values\n    vol_arr = df['volume'].values\n    ny_vwap_arr = df['ny_vwap'].values\n    slope_arr = df['slope'].values\n    vol_sma_arr = df['vol_sma'].values\n    upper_arr = df['upper'].values\n    lower_arr = df['lower'].values\n    \n    for i in range(len(df)):\n        ts = idx_arr[i]\n        t = ts.time()\n        d = ts.date()\n        \n        # --- NEW DAY LOGIC ---\n        if curr_date != d:\n            curr_date = d\n            orb_h, orb_l = np.nan, np.nan\n            broke_up, broke_dn = False, False\n            retest_ok = False\n            break_idx = 0\n            day_trades = 0\n            \n            # Force EOD VOID\n            if active:\n                active = None\n        \n        # --- VOID DATE CHECK ---\n        if d in VOID_DATES:\n            continue\n\n        # ORB\n        if TIME_ORB_START &lt;= t &lt; TIME_ORB_END:\n            orb_h = high_arr[i] if np.isnan(orb_h) else max(orb_h, high_arr[i])\n            orb_l = low_arr[i] if np.isnan(orb_l) else min(orb_l, low_arr[i])\n            continue\n        if np.isnan(orb_h): continue\n        orb_rng = orb_h - orb_l\n\n        # Break/Retest\n        if high_arr[i] &gt; orb_h and not broke_up:\n            broke_up = True; break_idx = i; retest_ok = False\n        if low_arr[i] &lt; orb_l and not broke_dn:\n            broke_dn = True; break_idx = i; retest_ok = False\n            \n        if broke_up and low_arr[i] &lt;= orb_h: retest_ok = True\n        if broke_dn and high_arr[i] &gt;= orb_l: retest_ok = True\n\n        # Exits\n        if active:\n            exit_px = None; reason = \"\"\n            if t &gt;= TIME_EOD_EXIT:\n                exit_px = close_arr[i]; reason = \"EOD\"\n            elif active['type'] == 'long':\n                if low_arr[i] &lt;= active['sl']: exit_px = active['sl']; reason = \"SL\"\n                elif high_arr[i] &gt;= active['tp']: exit_px = active['tp']; reason = \"TP\"\n                elif ENABLE_NY_VWAP_EXIT and close_arr[i] &lt; ny_vwap_arr[i]: exit_px = close_arr[i]; reason = \"VWAP\"\n            else:\n                if high_arr[i] &gt;= active['sl']: exit_px = active['sl']; reason = \"SL\"\n                elif low_arr[i] &lt;= active['tp']: exit_px = active['tp']; reason = \"TP\"\n                elif ENABLE_NY_VWAP_EXIT and close_arr[i] &gt; ny_vwap_arr[i]: exit_px = close_arr[i]; reason = \"VWAP\"\n\n            if not exit_px and ENABLE_5M_EXIT:\n                if 0 &lt; (i - active['ent_idx']) &lt;= 5 and t.minute % 5 == 4:\n                    if i &gt;= 4:\n                        op_5m = open_arr[i-4]\n                        if active['type'] == 'long' and close_arr[i] &lt; op_5m: exit_px = close_arr[i]; reason = \"5m\"\n                        elif active['type'] == 'short' and close_arr[i] &gt; op_5m: exit_px = close_arr[i]; reason = \"5m\"\n\n            if exit_px:\n                diff = (exit_px - active['ent_px']) if active['type'] == 'long' else (active['ent_px'] - exit_px)\n                pnl = (diff * active['qty'] * POINT_VALUE) - (COMMISSION_PER_CONTRACT * active['qty'])\n                \n                # UPDATE RUNNING EQUITY\n                current_equity += pnl\n                \n                trades.append({**active, 'exit_px': exit_px, 'exit_ts': ts, 'reason': reason, 'pnl': pnl})\n                active = None\n            continue\n\n        # Entries\n        if not (TIME_TRADE_START &lt;= t &lt; TIME_TRADE_END): continue\n        if day_trades &gt;= MAX_TRADES_PER_DAY: continue\n        \n        c_long = (broke_up and retest_ok and i &gt; break_idx and close_arr[i] &gt; orb_h and low_arr[i] &lt;= orb_h)\n        c_short = (broke_dn and retest_ok and i &gt; break_idx and close_arr[i] &lt; orb_l and high_arr[i] &gt;= orb_l)\n        \n        if not c_long and not c_short: continue\n        if not (lower_arr[i] &lt; close_arr[i] &lt; upper_arr[i]): continue\n        slp = slope_arr[i]\n        if np.isnan(slp): continue\n        vol_chk = vol_arr[i] &gt; (vol_sma_arr[i] * (VOL_MULT_LONG if c_long else VOL_MULT_SHORT))\n        if not vol_chk: continue\n\n        if c_long and slp &gt; 0 and close_arr[i] &gt; ny_vwap_arr[i] and close_arr[i] &lt; orb_h + orb_rng:\n            base_q = get_base_qty(t)\n            \n            # --- SCALING LOGIC ---\n            scale = BASELINE_SCALE_FACTOR\n            if ENABLE_DYNAMIC_SIZING:\n                # Ratio: Current / Initial\n                ratio = BASELINE_SCALE_FACTOR * current_equity / INITIAL_CAPITAL\n                # Floor to nearest 0.1 increment\n                scale = math.floor(ratio * 10) / 10.0\n                # Sanity check: Don't go below 0.1 scale\n                scale = max(0.1, scale)\n                \n            q = base_q * scale\n            \n            if q &gt; 0:\n                active = {'type':'long', 'ent_ts':ts, 'ent_px':close_arr[i], 'qty':q, \n                          'sl':orb_l, 'tp':orb_h+orb_rng, 'ent_idx':i, 'scale':scale}\n                day_trades += 1; broke_up = False; retest_ok = False\n\n        elif c_short and slp &lt; 0 and close_arr[i] &lt; ny_vwap_arr[i] and close_arr[i] &gt; orb_l - orb_rng:\n            base_q = get_base_qty(t)\n            \n            scale = BASELINE_SCALE_FACTOR\n            if ENABLE_DYNAMIC_SIZING:\n                ratio = BASELINE_SCALE_FACTOR * current_equity / INITIAL_CAPITAL\n                scale = math.floor(ratio * 10) / 10.0\n                scale = max(0.1, scale)\n            \n            q = base_q * scale\n            \n            if q &gt; 0:\n                active = {'type':'short', 'ent_ts':ts, 'ent_px':close_arr[i], 'qty':q, \n                          'sl':orb_h, 'tp':orb_l-orb_rng, 'ent_idx':i, 'scale':scale}\n                day_trades += 1; broke_dn = False; retest_ok = False\n\n    if len(trades) &gt; 0:\n        res = pd.DataFrame(trades)\n        # Recalculate full equity curve from PnL for analysis consistency\n        res['equity'] = res['pnl'].cumsum() + INITIAL_CAPITAL\n        \n        print(\"\\nExporting trades to 'kintoun_trade_log.csv'...\")\n        res.to_csv(\"kintoun_trade_log.csv\", index=False)\n        \n        analyze_performance(res, df)\n    else:\n        print(\"No trades generated.\")"
  },
  {
    "objectID": "project_kintoun.html#performance-analysis",
    "href": "project_kintoun.html#performance-analysis",
    "title": "Project Kintoun: A Regime-Filtered ORB Strategy",
    "section": "5. Performance Analysis",
    "text": "5. Performance Analysis\nBelow are the results generated from the simulation running on 1-minute NQ bars over the last ~3 years.\n\n\nCode\nrun_strategy()\n\n\nLoading NQ_1m.csv...\nCalculating Indicators...\nFiltering data from 2022-06-01 to 2025-11-01...\nSimulation Range: 2022-06-01 00:00:00-04:00 to 2025-10-31 16:59:00-04:00\n\nExporting trades to 'kintoun_trade_log.csv'...\n\n=============================================\n KINTOUN PERFORMANCE (2022-06-01 to 2025-11-01)\n Dynamic Sizing: True\n=============================================\nTotal Trades:      1062\nWin Rate:          33.90%\nProfit Factor:     1.47\n---------------------------------------------\nTotal PnL:         $404,294.74\nFinal Equity:      $604,294.74\nAnnualized Ret:    57.24%\n---------------------------------------------\nMax Drawdown:      -8.46% ($-22,569.99)\nSharpe Ratio:      2.28\nSortino Ratio:     4.00\nCalmar Ratio:      6.76\nTreynor Ratio:     2.36 (Beta: 0.23)\n=============================================\n\n[EXIT REASON DISTRIBUTION]\nreason\nVWAP    495\n5m      251\nTP      241\nEOD      67\nSL        8\nName: count, dtype: int64"
  },
  {
    "objectID": "project_kintoun.html#conclusion-future-work",
    "href": "project_kintoun.html#conclusion-future-work",
    "title": "Project Kintoun: A Regime-Filtered ORB Strategy",
    "section": "6. Conclusion & Future Work",
    "text": "6. Conclusion & Future Work\nThe Kintoun Strategy demonstrates significantly improved expectancy and risk-adjusted return, as well as lower max drawdown compared to a naive breakout strategy. Using Volume, VWAP, and slope ensures we are not trading against the dominant intraday flow.\nFuture Improvements:\n- Execution: Moving from close prices to Tick Data to better model slippage on stop orders.\n- Optimization: Exploring a machine learning approach (Random Forest, XGBoost) to classify the probability of a successful retest/trade."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jeremy Hsu",
    "section": "",
    "text": "I am a freshman at Penn State University double majoring in Computer Science and Statistics.\nI am interested in the combination of Software Engineering and Statistics/Data Science in Financial Markets. My goal is to build robust, event-driven trading systems and analyze market microstructure using quantitative approaches.\nCurrently, I am researching meta filtration and machine learning optimization in trading models and building my own automated trading system in Python."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jeremy Hsu",
    "section": "",
    "text": "I am a freshman at Penn State University double majoring in Computer Science and Statistics.\nI am interested in the combination of Software Engineering and Statistics/Data Science in Financial Markets. My goal is to build robust, event-driven trading systems and analyze market microstructure using quantitative approaches.\nCurrently, I am researching meta filtration and machine learning optimization in trading models and building my own automated trading system in Python."
  },
  {
    "objectID": "index.html#featured-research",
    "href": "index.html#featured-research",
    "title": "Jeremy Hsu",
    "section": "Featured Research",
    "text": "Featured Research\n\nProject Kintoun: NQ Futures Strategy\nStatus: Completed (Dec 2025)\nAn event-driven algorithmic trading strategy designed for the Nasdaq-100 E-Mini Futures (NQ). This project explores how Market Microstructure (Break & Retest) combined with Volume and Regression Analysis can filter out false breakouts in variable volatility environments.\n\nTechnique: Event-driven simulation (partially vectorized).\nData Handling: Timezone-aligned, degradation-filtered 1-minute OHLCV data.\nKey Metric: significantly improved expectancy (Profit Factor) and risk-adjusted return (Sharpe) compared to naive ORB strategies.\n\nView the full Analysis & Backtest ➞"
  },
  {
    "objectID": "index.html#technical-stack",
    "href": "index.html#technical-stack",
    "title": "Jeremy Hsu",
    "section": "Technical Stack",
    "text": "Technical Stack\n\n\n\nDomain\nSkills\n\n\n\n\nLanguages\nPython, C++, Java, R\n\n\nLibraries\nPandas, NumPy, Scikit-Learn, MatPlotLib\n\n\nQuant\nTime-Series Data Analysis, Backtesting, Risk Management\n\n\nTools\nGit, VS Code, Jupyter, Linux/Ubuntu, Quarto"
  }
]